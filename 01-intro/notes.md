## Использование LLM для задачи RAG

LLM - large language model. Они содержать милиарды параметров, они учились на огромных наборах данных, но если 
спросить их вопрос, относящийся к нашим данных, то скорее всего они не поймут о чем идет речь. 
Для ответа на наш вопрос им нужно дать контекст, в котором они могут найти этот ответ.

## В этом и есть суть задачи RAG.

**RAG** - retrieval-augmented generation - это задача, в которой система должна найти подходящий контекст к ответу, а затем сгенерировать хороший ответ на вопрос.

В качестве данных мы будем использовать FAQ документ из других курсов DataTalksClub, в частности курсы ML Zoomcamp, DE Zoomcamp.

Компоненты **RAG**:
* **Retriever** - поиск подходящего контекста (search engine)
* **Generator** - генерация ответа (llm)

## Retriever (search engine)

Можно использовать разные подходы, чтобы найти в нашем FAQ подходящие ответы на вопрос.  
Сначала, мы попробуем использовать простой поисковый движок, который Алексей сделал на вводном вебинаре.  
Потом мы будем менять используемые engines.

## Generator (llm)

Мы будем рассматривать LLM как черный ящик, который принимает на вход "промпт" и генерирует текст ответа.  
Наш промт будет выглядеть примерно так:
```
question: [вопрос]
contexts: [ответ1, ответ2, ...]
answer: 
```

Слова в квадратных скобках будут заменены на реальные вопрос и ответы из FAQ.

В качестве модели я буду использовать api Anthropic claude.

Также можно использовать другие модели, например, Hugging Face или OpenAI.






